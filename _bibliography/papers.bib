---
---

@@inproceedings{oh_stable_2024,
title        = {Stable {Neural} {Stochastic} {Differential} {Equations} in {Analyzing} {Irregular} {Time} {Series} {Data}},
author       = {Oh, YongKyung and Lim, Dongyoung and Kim, Sungil},
year         = 2024,
booktitle    = {The {Twelfth} {International} {Conference} on {Learning} {Representations}, {ICLR} 2024, {Vienna}, {Austria}, {May} 7-11, 2024},
publisher    = {OpenReview.net},
abbr={NSDEs},
abstract={Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.},
paper={https://openreview.net/forum?id=4VIgNuQ1pY},
arxiv={2402.14989},
github={yongkyung-oh/Stable-Neural-SDEs},
}


@inproceedings{oh_dualdynamics_2025,
title        = {{DualDynamics}: {Synergizing} {Implicit} and {Explicit} {Methods} for {Robust} {Irregular} {Time} {Series} {Analysis}},
shorttitle   = {{DualDynamics}},
author       = {Oh, YongKyung and Lim, Dong-Young and Kim, Sungil},
year         = 2025,
booktitle    = {{AAAI}-25, {Sponsored} by the {Association} for the {Advancement} of {Artificial} {Intelligence}, {February} 25 - {March} 4, 2025, {Philadelphia}, {PA}, {USA}},
publisher    = {AAAI Press},
pages        = {19730--19739},
doi          = {10.1609/AAAI.V39I18.34173},
editor       = {Walsh, Toby and Shah, Julie and Kolter, Zico},
abbr={NCDEs},
abstract={Real-world time series analysis faces significant challenges when dealing with irregular and incomplete data. While Neural Differential Equation (NDE) based methods have shown promise, they struggle with limited expressiveness, scalability issues, and stability concerns. Conversely, Neural Flows offer stability but falter with irregular data. We introduce 'DualDynamics', a novel framework that synergistically combines NDE-based method and Neural Flow-based method. This approach enhances expressive power while balancing computational demands, addressing critical limitations of existing techniques. We demonstrate DualDynamics' effectiveness across diverse tasks: classification of robustness to dataset shift, irregularly-sampled series analysis, interpolation of missing data, and forecasting with partial observations. Our results show consistent outperformance over state-of-the-art methods, indicating DualDynamics' potential to advance irregular time series analysis significantly.},
paper={https://ojs.aaai.org/index.php/AAAI/article/view/34173},
arxiv={2401.04979},
github={yongkyung-oh/DualDynamics},
}

@inproceedings{rubanova_latent_2019,
title        = {Latent {Ordinary} {Differential} {Equations} for {Irregularly}-{Sampled} {Time} {Series}.},
author       = {Rubanova, Yulia and Chen, Tian Qi and Duvenaud, David},
year         = 2019,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}.},
pages        = {5321--5331},
abbr={NODEs},
abstract={Many real-world time series are irregularly-sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks are not well-suited to this setting, as they assume fixed time intervals between observations. We propose a new model for irregularly-sampled time series based on Neural Ordinary Differential Equations (ODEs). Our model learns the continuous-time dynamics of the underlying system, and can be evaluated at arbitrary time points. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data. Our model outperforms state-of-the-art methods for irregularly-sampled time series, and is competitive with methods for regularly-sampled data.},
paper={https://proceedings.neurips.cc/paper_files/paper/2019/file/432a7e5a8e4864265426e6219927a69e-Paper.pdf},
arxiv={1907.03907},
github={YuliaRubanova/latent_ode},
}

@inproceedings{kidger_neural_2020,
title        = {Neural {Controlled} {Differential} {Equations} for {Irregular} {Time} {Series}.},
author       = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry J.},
year         = 2020,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual.},
abbr={NCDEs},
abstract={Many real-world time series are irregularly sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks are not well-suited to this setting, as they assume fixed time intervals between observations. We propose a new model for irregularly-sampled time series based on Neural Controlled Differential Equations (CDEs). Our model learns the continuous-time dynamics of the underlying system, and can be evaluated at arbitrary time points. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data. Our model outperforms state-of-the-art methods for irregularly-sampled time series, and is competitive with methods for regularly-sampled data.},
paper={https://proceedings.neurips.cc/paper/2020/file/6e1366887956979f3326740908976477-Paper.pdf},
arxiv={2005.08926},
github={patrickkidger/NeuralCDE},
}

@inproceedings{li_scalable_2020,
title        = {Scalable {Gradients} for {Stochastic} {Differential} {Equations}},
author       = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
year         = 2020,
month        = aug,
booktitle    = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
publisher    = {PMLR},
series       = {Proceedings of {Machine} {Learning} {Research}},
volume       = 108,
pages        = {3870--3882},
editor       = {Chiappa, Silvia and Calandra, Roberto},
abbr={NSDEs},
abstract={Stochastic differential equations (SDEs) are a powerful tool for modeling systems with inherent noise. However, training models that involve SDEs can be challenging due to the difficulty of computing gradients through stochastic processes. We propose a new method for computing gradients of expectations with respect to SDEs that is both scalable and unbiased. Our method leverages the adjoint method, which allows us to compute gradients by solving a backward SDE. We demonstrate the effectiveness of our method on several tasks, including training generative models and learning latent SDEs from data. Our method outperforms existing approaches in terms of both accuracy and computational efficiency.},
paper={http://proceedings.mlr.press/v108/li20a/li20a.pdf},
arxiv={2002.09544},
github={google-research/torchsde},
}

@inproceedings{morrill_neural_2021,
title        = {Neural {Rough} {Differential} {Equations} for {Long} {Time} {Series}.},
author       = {Morrill, James and Salvi, Cristopher and Kidger, Patrick and Foster, James},
year         = 2021,
booktitle    = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}.},
pages        = {7829--7838},
abbr={NCDEs},
abstract={Neural Controlled Differential Equations (CDEs) are a powerful model for irregularly-sampled time series. However, their computational cost scales quadratically with the length of the time series, making them impractical for long sequences. We propose Neural Rough Differential Equations (RDEs), a new model that leverages the theory of rough paths to reduce the computational cost of CDEs to linear in the length of the time series. Our model retains the expressiveness of CDEs while being significantly more efficient. We demonstrate the effectiveness of our model on several real-world datasets, including human motion capture data and medical records. Our model outperforms existing methods for long irregularly-sampled time series.},
paper={http://proceedings.mlr.press/v139/morrill21a/morrill21a.pdf},
arxiv={2106.01311},
github={patrickkidger/NeuralRDE},
}

@inproceedings{jhin_exit_2022,
title        = {{EXIT}: {Extrapolation} and {Interpolation}-based {Neural} {Controlled} {Differential} {Equations} for {Time}-series {Classification} and {Forecasting}.},
author       = {Jhin, Sheo Yon and Lee, Jaehoon and Jo, Minju and Kook, Seungji and Jeon, Jinsung and Hyeong, Jihyeon and Kim, Jayoung and Park, Noseong},
year         = 2022,
booktitle    = {{WWW} '22: {The} {ACM} {Web} {Conference} 2022, {Virtual} {Event}, {Lyon}, {France}, {April} 25 - 29, 2022},
pages        = {3102--3112},
doi          = {10.1145/3485447.3512030},
abbr={NCDEs},
abstract={Neural Controlled Differential Equations (NCDEs) have emerged as a powerful tool for modeling irregularly sampled time series data. However, existing NCDE models often struggle with extrapolation and interpolation tasks, which are crucial for real-world applications like time-series classification and forecasting. To address these limitations, we propose EXIT (Extrapolation and Interpolation-based Neural Controlled Differential Equations), a novel framework that enhances NCDEs' capabilities in handling both extrapolation and interpolation. EXIT introduces a dual-path architecture that explicitly models both the observed data points and the underlying continuous dynamics, allowing for more robust and accurate predictions. We demonstrate the effectiveness of EXIT on various real-world datasets, showing significant improvements over state-of-the-art NCDE models in time-series classification and forecasting tasks, especially in scenarios with high irregularity and missing data.},
paper={https://dl.acm.org/doi/abs/10.1145/3485447.3512030},
arxiv={2203.09020},
github={sheoyon-jhin/EXIT},
}

@article{jhin_learnable_2023,
title        = {Learnable {Path} in {Neural} {Controlled} {Differential} {Equations}},
author       = {Jhin, Sheo Yon and Jo, Minju and Kook, Seungji and Park, Noseong},
year         = 2023,
month        = jun,
journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
volume       = 37,
number       = 7,
pages        = {8014--8022},
doi          = {10.1609/aaai.v37i7.25969},
abbr={NCDEs},
abstract={Neural Controlled Differential Equations (NCDEs) have shown great promise in modeling irregularly sampled time series data. A key component of NCDEs is the path, which typically relies on linear or cubic spline interpolation of the input data. However, these fixed interpolation schemes may not be optimal for capturing complex underlying dynamics. In this paper, we propose a novel approach that introduces a 'learnable path' within NCDEs. Our method allows the model to dynamically learn the most suitable interpolation scheme for the given data, thereby enhancing its expressiveness and adaptability. We demonstrate the effectiveness of our learnable path NCDE on various real-world datasets, showing improved performance in tasks such as time-series classification and forecasting, especially in scenarios with highly irregular observations.},
paper={https://ojs.aaai.org/index.php/AAAI/article/view/25969},
arxiv={2301.08502},
github={sheoyon-jhin/Learnable-Path-NCDE},
}

@inproceedings{dupont_augmented_2019,
title        = {Augmented {Neural} {ODEs}.},
author       = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
year         = 2019,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}.},
pages        = {3134--3144},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) parameterize the derivative of the hidden state using a neural network. This allows them to model continuous-time dynamics and handle irregularly sampled data. However, Neural ODEs are limited by the fact that the hidden state must have the same dimension as the input. We propose Augmented Neural ODEs (ANODEs), which augment the hidden state with additional dimensions. This allows ANODEs to learn more complex dynamics and improve performance on tasks such as density estimation and time series modeling. We demonstrate the effectiveness of ANODEs on several real-world datasets, showing significant improvements over standard Neural ODEs.},
paper={https://proceedings.neurips.cc/paper/2019/file/2d107313a774780753067311b739955e-Paper.pdf},
arxiv={1904.01681},
github={EmilienDupont/augmented-neural-odes},
}

@inproceedings{brouwer_gru-ode-bayes_2019,
title        = {{GRU}-{ODE}-{Bayes}: {Continuous} {Modeling} of {Sporadically}-{Observed} {Time} {Series}.},
author       = {Brouwer, Edward De and Simm, Jaak and Arany, Adam and Moreau, Yves},
year         = 2019,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}.},
pages        = {7377--7388},
abbr={NODEs},
abstract={Many real-world time series are sporadically observed, meaning that observations arrive at irregular time intervals and some values may be missing. Standard recurrent neural networks struggle with such data due to their reliance on fixed time steps. We propose GRU-ODE-Bayes, a novel model that combines the strengths of Gated Recurrent Units (GRUs) with Ordinary Differential Equations (ODEs) and Bayesian inference to handle sporadically observed time series. Our model learns continuous-time dynamics and provides uncertainty estimates for predictions. We demonstrate the effectiveness of GRU-ODE-Bayes on several real-world datasets, including medical records and climate data, showing significant improvements over state-of-the-art methods for irregularly sampled time series.},
paper={https://proceedings.neurips.cc/paper/2019/file/7377200700f909599e03099ad1139cb3-Paper.pdf},
arxiv={1905.13221},
github={edebrouwer/GRU-ODE-Bayes},
}

@misc{oh_comprehensive_2025,
title        = {Comprehensive {Review} of {Neural} {Differential} {Equations} for {Time} {Series} {Analysis}},
author       = {Oh, YongKyung and Kam, Seungsu and Lee, Jonghun and Lim, Dong-Young and Kim, Sungil and Bui, Alex},
year         = 2025,
publisher    = {arXiv},
doi          = {10.48550/arXiv.2502.09885},
abbr={Review},
abstract={Neural Differential Equations (NDEs) have emerged as a powerful paradigm for modeling complex dynamic systems, particularly in the context of time series analysis. This comprehensive review provides an in-depth exploration of the various NDE architectures, including Neural Ordinary Differential Equations (NODEs), Neural Controlled Differential Equations (NCDEs), and Neural Stochastic Differential Equations (NSDEs), and their applications in handling irregularly sampled, noisy, and high-dimensional time series data. We discuss the theoretical foundations, computational challenges, and practical considerations for implementing and training NDE-based models. Furthermore, we highlight recent advancements, open problems, and future research directions in this rapidly evolving field, aiming to provide a valuable resource for researchers and practitioners interested in leveraging NDEs for advanced time series analysis.},
paper={https://arxiv.org/abs/2502.09885},
arxiv={2502.09885},
}

@misc{lechner_learning_2020,
title        = {Learning {Long}-{Term} {Dependencies} in {Irregularly}-{Sampled} {Time} {Series}},
author       = {Lechner, Mathias and Hasani, Ramin},
year         = 2020,
month        = dec,
publisher    = {arXiv},
doi          = {10.48550/arXiv.2006.04418},
abbr={NODEs},
abstract={Many real-world time series are irregularly sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks struggle with such data due to their reliance on fixed time steps. We propose a novel approach for learning long-term dependencies in irregularly-sampled time series by combining the strengths of recurrent neural networks with continuous-time models. Our method leverages a continuous-time hidden state that evolves according to a learned ordinary differential equation, allowing it to capture complex temporal dynamics regardless of sampling irregularity. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data, showing improved performance in tasks such as classification and forecasting.},
paper={https://arxiv.org/abs/2006.04418},
arxiv={2006.04418},
github={mlech26l/ode-lstms},
}

@misc{tzen_neural_2019,
title        = {Neural {Stochastic} {Differential} {Equations}: {Deep} {Latent} {Gaussian} {Models} in the {Diffusion} {Limit}},
shorttitle   = {Neural {Stochastic} {Differential} {Equations}},
author       = {Tzen, Belinda and Raginsky, Maxim},
year         = 2019,
month        = oct,
publisher    = {arXiv},
doi          = {10.48550/arXiv.1905.09883},
abbr={NSDEs},
abstract={We introduce Neural Stochastic Differential Equations (Neural SDEs), a new class of deep generative models that extend Neural Ordinary Differential Equations (Neural ODEs) by incorporating a stochastic diffusion term. Neural SDEs model continuous-time dynamics with inherent noise, allowing for more robust and expressive representations of complex data. We show that Neural SDEs can be viewed as deep latent Gaussian models in the diffusion limit, providing a theoretical foundation for their generative capabilities. We demonstrate the effectiveness of Neural SDEs on tasks such as density estimation and time series modeling, showing competitive performance with state-of-the-art methods while offering improved flexibility and interpretability.},
paper={https://arxiv.org/abs/1905.09883},
arxiv={1905.09883},
}

@inproceedings{yildiz_ode2vae_2019,
title        = {{ODE2VAE}: {Deep} generative second order {ODEs} with {Bayesian} neural networks},
author       = {Yildiz, Cagatay and Heinonen, Markus and Lahdesmaki, Harri},
year         = 2019,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 32,
editor       = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
abbr={NODEs},
abstract={Deep generative models have achieved remarkable success in various domains, but modeling complex temporal dynamics, especially with irregularly sampled data, remains a challenge. We propose ODE2VAE, a novel deep generative model that combines second-order Ordinary Differential Equations (ODEs) with Variational Autoencoders (VAEs) and Bayesian neural networks. Our model learns continuous-time dynamics in the latent space, allowing it to handle irregular observations and capture complex temporal dependencies. The use of second-order ODEs enables modeling of acceleration and momentum, providing richer dynamics. We demonstrate the effectiveness of ODE2VAE on several real-world datasets, including medical records and motion capture data, showing improved performance in tasks such as imputation, forecasting, and anomaly detection.},
paper={https://proceedings.neurips.cc/paper/2019/file/202cb962ac59075b964b07152d234b70-Paper.pdf},
github={cagatayyildiz/ODE2VAE},
}

@inproceedings{ghosh_steer_2020,
title        = {{STEER} : {Simple} {Temporal} {Regularization} {For} {Neural} {ODE}},
author       = {Ghosh, Arnab and Behl, Harkirat and Dupont, Emilien and Torr, Philip and Namboodiri, Vinay},
year         = 2020,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 33,
pages        = {14831--14843},
editor       = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) have emerged as a powerful tool for modeling continuous-time dynamics. However, training Neural ODEs can be challenging due to their sensitivity to initial conditions and the lack of explicit regularization for temporal smoothness. We propose STEER (Simple Temporal Regularization For Neural ODE), a novel regularization technique that encourages temporal smoothness in the learned dynamics of Neural ODEs. STEER adds a penalty term to the loss function that regularizes the magnitude of the derivative of the hidden state, promoting more stable and interpretable dynamics. We demonstrate the effectiveness of STEER on various tasks, including time series modeling and density estimation, showing improved performance and robustness compared to standard Neural ODEs.},
paper={https://proceedings.neurips.cc/paper/2020/file/58d4d1e7b1e97b252200c512f91e0edb-Paper.pdf},
}

@misc{liu_neural_2019,
title        = {Neural {SDE}: {Stabilizing} {Neural} {ODE} {Networks} with {Stochastic} {Noise}},
shorttitle   = {Neural {SDE}},
author       = {Liu, Xuanqing and Xiao, Tesi and Si, Si and Cao, Qin and Kumar, Sanjiv and Hsieh, Cho-Jui},
year         = 2019,
publisher    = {arXiv},
doi          = {10.48550/ARXIV.1906.02355},
abbr={NSDEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) have gained popularity for modeling continuous-time dynamics. However, they can be sensitive to noise and prone to instability, especially in high-dimensional settings. We propose Neural SDE, a novel approach that stabilizes Neural ODE networks by introducing stochastic noise into the dynamics. By augmenting the deterministic ODE with a learnable diffusion term, Neural SDE can better capture uncertainties and improve robustness. We demonstrate the effectiveness of Neural SDE on various tasks, including density estimation and time series modeling, showing improved stability and performance compared to standard Neural ODEs, particularly in the presence of noisy data.},
paper={https://arxiv.org/abs/1906.02355},
arxiv={1906.02355},
}

@inproceedings{zhuang_adaptive_2020,
title        = {Adaptive {Checkpoint} {Adjoint} {Method} for {Gradient} {Estimation} in {Neural} {ODE}},
author       = {Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, Sekhar and Papademetris, Xenophon and Duncan, James},
year         = 2020,
month        = jul,
booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
publisher    = {PMLR},
series       = {Proceedings of {Machine} {Learning} {Research}},
volume       = 119,
pages        = {11639--11649},
editor       = {III, Hal Daumé and Singh, Aarti},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) offer a memory-efficient way to model continuous-time dynamics by backpropagating through an ODE solver using the adjoint method. However, the adjoint method can be computationally expensive, especially for long integration times, as it requires solving an additional backward ODE. We propose the Adaptive Checkpoint Adjoint Method (ACAM), a novel approach that combines checkpointing with the adjoint method to reduce memory and computational costs. ACAM adaptively selects checkpoints during the forward pass, allowing for efficient gradient computation without storing the entire trajectory. We demonstrate the effectiveness of ACAM on various tasks, showing significant reductions in memory and computation while maintaining accuracy compared to standard adjoint methods.},
paper={http://proceedings.mlr.press/v119/zhuang20a/zhuang20a.pdf},
arxiv={2006.02448},
}

@inproceedings{zhang_approximation_2020,
title        = {Approximation {Capabilities} of {Neural} {ODEs} and {Invertible} {Residual} {Networks}},
author       = {Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
year         = 2020,
month        = jul,
booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
publisher    = {PMLR},
series       = {Proceedings of {Machine} {Learning} {Research}},
volume       = 119,
pages        = {11086--11095},
editor       = {III, Hal Daumé and Singh, Aarti},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) and Invertible Residual Networks (i-ResNets) are two recent architectures that offer memory-efficient training and exact likelihood computation, respectively. Both models are based on continuous-depth transformations. We investigate the approximation capabilities of Neural ODEs and i-ResNets, showing that they are universal approximators of continuous functions. We provide theoretical guarantees on their ability to approximate arbitrary continuous functions and demonstrate their practical performance on various tasks, including density estimation and image classification. Our results shed light on the expressive power of these continuous-depth models and their potential for deep learning applications.},
paper={http://proceedings.mlr.press/v119/zhang20c/zhang20c.pdf},
arxiv={2006.02448},
}

@inproceedings{jia_neural_2019,
title        = {Neural {Jump} {Stochastic} {Differential} {Equations}},
author       = {Jia, Junteng and Benson, Austin R},
year         = 2019,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 32,
editor       = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
abbr={NSDEs},
abstract={Stochastic differential equations (SDEs) are widely used to model systems with continuous-time random fluctuations. However, many real-world phenomena exhibit sudden, discrete changes or jumps, which are not captured by standard SDEs. We introduce Neural Jump Stochastic Differential Equations (Neural JSDEs), a novel class of models that extend Neural SDEs by incorporating jump processes. Neural JSDEs can model both continuous diffusion and discrete jumps, providing a more comprehensive framework for complex temporal dynamics. We demonstrate the effectiveness of Neural JSDEs on various tasks, including financial modeling and anomaly detection, showing improved performance in scenarios with abrupt changes.},
paper={https://proceedings.neurips.cc/paper/2019/file/21c7069d0ee8779496879895-Paper.pdf},
arxiv={1905.09883},
}

@article{e_deep_2017,
title        = {Deep {Learning}-{Based} {Numerical} {Methods} for {High}-{Dimensional} {Parabolic} {Partial} {Differential} {Equations} and {Backward} {Stochastic} {Differential} {Equations}},
author       = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
year         = 2017,
month        = dec,
journal      = {Communications in Mathematics and Statistics},
volume       = 5,
number       = 4,
pages        = {349--380},
doi          = {10.1007/s40304-017-0117-6},
abbr={NSDEs},
abstract={We propose a deep learning-based numerical method for solving high-dimensional parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs). The method is based on approximating the unknown function by a deep neural network and training the network by minimizing a loss function derived from the PDE or BSDE. We demonstrate the effectiveness of our method on several high-dimensional examples, including the Black-Scholes equation and a high-dimensional Hamilton-Jacobi-Bellman equation. Our method outperforms traditional numerical methods in terms of accuracy and computational efficiency for high-dimensional problems.},
paper={https://link.springer.com/article/10.1007/s40304-017-0117-6},
arxiv={1707.02568},
}

@article{morrill_choice_2022,
title        = {On the {Choice} of {Interpolation} {Scheme} for {Neural} {CDEs}},
author       = {Morrill, James and Kidger, Patrick and Yang, Lingyi and Lyons, Terry},
year         = 2022,
journal      = {Transactions on Machine Learning Research},
abbr={NCDEs},
abstract={Neural Controlled Differential Equations (Neural CDEs) are a powerful model for irregularly-sampled time series. A key component of Neural CDEs is the choice of interpolation scheme for the input data. Common choices include linear interpolation and cubic spline interpolation. We investigate the impact of different interpolation schemes on the performance of Neural CDEs. We show that the choice of interpolation scheme can significantly affect the model's ability to capture complex dynamics and generalize to unseen data. We provide theoretical insights and empirical evidence on various real-world datasets, demonstrating that a carefully chosen interpolation scheme can lead to substantial performance improvements.},
paper={https://jmlr.org/papers/v23/21-0902.html},
arxiv={2109.00891},
}

@misc{teshima_universal_2020,
title        = {Universal {Approximation} {Property} of {Neural} {Ordinary} {Differential} {Equations}},
author       = {Teshima, Takeshi and Tojo, Koichi and Ikeda, Masahiro and Ishikawa, Isao and Oono, Kenta},
year         = 2020,
month        = dec,
publisher    = {arXiv},
doi          = {10.48550/arXiv.2012.02414},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) have recently emerged as a new class of deep learning models that parameterize the derivative of the hidden state using a neural network. This allows them to model continuous-time dynamics and offer memory-efficient training. We prove that Neural ODEs possess the universal approximation property, meaning they can approximate any continuous function to arbitrary accuracy. Our proof provides theoretical guarantees on the expressive power of Neural ODEs and sheds light on their ability to learn complex mappings. This result further solidifies the theoretical foundation of Neural ODEs and supports their wide applicability in various domains.},
paper={https://arxiv.org/abs/2012.02414},
arxiv={2012.02414},
}

@inproceedings{behrmann_invertible_2019,
title        = {Invertible {Residual} {Networks}},
author       = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik},
year         = 2019,
booktitle    = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
publisher    = {PMLR},
address      = {Proceedings of Machine Learning Research},
volume       = 97,
pages        = {573--582},
editor       = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
abbr={NODEs},
abstract={We introduce Invertible Residual Networks (i-ResNets), a new class of deep neural networks that are invertible by construction. This property allows for exact likelihood computation and memory-efficient training, as activations do not need to be stored for backpropagation. i-ResNets are built upon the idea of residual blocks, where each block is designed to be invertible. We demonstrate the effectiveness of i-ResNets on various tasks, including image classification and density estimation, showing competitive performance with state-of-the-art models while offering significant memory savings. Our work opens up new possibilities for designing deep architectures with desirable properties.},
paper={http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf},
arxiv={1811.00995},
github={jhjacobsen/invertible-resnet},
}

@inproceedings{grathwohl_ffjord_2019,
title        = {{FFJORD}: {Free}-{Form} {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}.},
author       = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
year         = 2019,
booktitle    = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
abbr={NODEs},
abstract={Normalizing flows are a powerful class of generative models that allow for exact likelihood computation. However, existing normalizing flows often suffer from limitations in expressiveness or scalability. We introduce FFJORD (Free-Form Jacobian of Reversible Dynamics), a novel normalizing flow model based on Neural Ordinary Differential Equations (Neural ODEs). FFJORD parameterizes the transformation as a continuous-time dynamic system, allowing for flexible and expressive transformations while maintaining invertibility and exact likelihood computation. We demonstrate the scalability and effectiveness of FFJORD on various tasks, including density estimation and image generation, showing competitive performance with state-of-the-art generative models.},
paper={https://openreview.net/pdf?id=rJg6CVAcf7},
arxiv={1810.01367},
github={rtqichen/ffjord},
}

@article{jhin_attentive_2024,
title        = {Attentive neural controlled differential equations for time-series classification and forecasting},
author       = {Jhin, Sheo Yon and Shin, Heejoo and Kim, Sujie and Hong, Seoyoung and Jo, Minju and Park, Solhee and Park, Noseong and Lee, Seungbeom and Maeng, Hwiyoung and Jeon, Seungmin},
year         = 2024,
month        = mar,
journal      = {Knowledge and Information Systems},
volume       = 66,
number       = 3,
pages        = {1885--1915},
doi          = {10.1007/s10115-023-01977-5},
abbr={NCDEs},
abstract={Neural Controlled Differential Equations (NCDEs) have shown promising results in modeling irregularly sampled time series. However, their ability to capture long-range dependencies and focus on relevant information within the time series can be limited. We propose Attentive Neural Controlled Differential Equations (Attentive NCDEs), a novel framework that integrates attention mechanisms into NCDEs. This allows the model to dynamically weigh the importance of different time points and features, enhancing its ability to capture salient information for time-series classification and forecasting. We demonstrate the effectiveness of Attentive NCDEs on various real-world datasets, showing significant improvements over state-of-the-art NCDE models, especially in complex scenarios with varying data quality and length.},
paper={https://link.springer.com/article/10.1007/s10115-023-01977-5},
arxiv={2303.09020},
github={sheoyon-jhin/Attentive-NCDE},
}

@inproceedings{chen_neural_2018,
title        = {Neural {Ordinary} {Differential} {Equations}.},
author       = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
year         = 2018,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}.},
pages        = {6572--6583},
abbr={NODEs},
abstract={We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed by an off-the-shelf ODE solver. These 'Neural ODEs' can model continuous-time dynamics, and can be trained with backpropagation through the ODE solver. This allows for memory-efficient training, as the adjoint method can be used to compute gradients without storing the entire forward pass. We demonstrate the effectiveness of Neural ODEs on various tasks, including density estimation, time series modeling, and image classification, showing competitive performance with state-of-the-art models while offering significant memory savings.},
paper={https://proceedings.neurips.cc/paper/2018/file/6ad323483731627375b420a359b99a74-Paper.pdf},
arxiv={1806.07366},
github={rtqichen/torchdiffeq},
}

@inproceedings{song_score-based_2021,
title        = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}.},
author       = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
year         = 2021,
booktitle    = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
abbr={NSDEs},
abstract={We propose a unified framework for generative modeling that leverages stochastic differential equations (SDEs) to transform a simple prior distribution into a complex data distribution. Our framework encompasses various existing generative models, including score-based generative models and denoising diffusion probabilistic models, as special cases. By defining a continuous-time SDE that gradually perturbs data towards noise, and its reverse-time SDE that transforms noise back into data, we can generate samples by solving the reverse SDE. We demonstrate the effectiveness of our framework on various tasks, including image generation and unconditional sampling, showing competitive performance with state-of-the-art generative models.},
paper={https://openreview.net/pdf?id=PxTIG12RRf},
arxiv={2011.13456},
github={yang-song/score_sde},
}

@inproceedings{lou_neural_2020,
title        = {Neural {Manifold} {Ordinary} {Differential} {Equations}},
author       = {Lou, Aaron and Lim, Derek and Katsman, Isay and Huang, Leo and Jiang, Qingxuan and Lim, Ser Nam and De Sa, Christopher M},
year         = 2020,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 33,
pages        = {17548--17558},
editor       = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) have shown promise in modeling continuous-time dynamics. However, they operate in Euclidean space, which may not be suitable for data residing on non-Euclidean manifolds. We introduce Neural Manifold Ordinary Differential Equations (Neural Manifold ODEs), a novel framework that extends Neural ODEs to learn dynamics on Riemannian manifolds. Our approach leverages geometric deep learning techniques to define ODEs on curved spaces, allowing for more natural and expressive modeling of data with underlying manifold structures. We demonstrate the effectiveness of Neural Manifold ODEs on tasks such as learning dynamics on spheres and hyperbolic spaces, showing improved performance and interpretability compared to Euclidean counterparts.},
paper={https://proceedings.neurips.cc/paper/2020/file/229699d324817122c560356723229718-Paper.pdf},
arxiv={2006.08445},
github={louaaron/neural-manifold-odes},
}

@inproceedings{li_neural_2023,
title        = {Neural {Lad}: {A} {Neural} {Latent} {Dynamics} {Framework} for {Times} {Series} {Modeling}},
author       = {li, ting and Li, Jianguo and Zhu, Zhanxing},
year         = 2023,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 36,
pages        = {17345--17356},
editor       = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
abbr={NODEs},
abstract={Modeling complex time series data, especially those with irregular sampling and missing values, remains a significant challenge. Traditional methods often struggle with capturing underlying continuous dynamics and long-term dependencies. We propose Neural LAD (Neural Latent Dynamics), a novel framework that leverages latent ordinary differential equations (ODEs) to model the continuous evolution of time series in a low-dimensional latent space. Neural LAD learns a flexible, data-driven latent dynamics model that can handle irregular observations and infer missing values. We demonstrate the effectiveness of Neural LAD on various real-world datasets, including medical records and human activity recognition, showing superior performance in tasks such as imputation, forecasting, and classification compared to state-of-the-art methods.},
paper={https://proceedings.neurips.cc/paper_files/paper/2023/file/17345-Paper-Neural-Lad-A-Neural-Latent-Dynamics-Framework-for-Times-Series-Modeling-Abstract.pdf},
arxiv={2305.18470},
github={tingli-tsinghua/Neural-LAD},
}

@inproceedings{massaroli_dissecting_2020,
title        = {Dissecting {Neural} {ODEs}},
author       = {Massaroli, Stefano and Poli, Michael and Park, Jinkyoo and Yamashita, Atsushi and Asama, Hajime},
year         = 2020,
booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
publisher    = {Curran Associates, Inc.},
volume       = 33,
pages        = {3952--3963},
editor       = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) have gained significant attention for their ability to model continuous-time dynamics and offer memory-efficient training. However, their internal workings and the factors influencing their performance are not yet fully understood. We conduct a thorough dissection of Neural ODEs, analyzing various aspects such as their approximation capabilities, sensitivity to hyperparameters, and the role of the ODE solver. Through extensive empirical studies and theoretical insights, we provide a deeper understanding of why Neural ODEs work and how to effectively train them. Our findings offer practical guidelines for designing and applying Neural ODEs in various machine learning tasks.},
paper={https://proceedings.neurips.cc/paper/2020/file/3952-Paper.pdf},
arxiv={2006.07366},
github={massaroli/Dissecting-Neural-ODEs},
}

@inproceedings{gholaminejad_anode_2019,
title        = {{ANODE}: {Unconditionally} {Accurate} {Memory}-{Efficient} {Gradients} for {Neural} {ODEs}.},
author       = {Gholaminejad, Amir and Keutzer, Kurt and Biros, George},
year         = 2019,
booktitle    = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}, {IJCAI} 2019, {Macao}, {China}, {August} 10-16, 2019},
pages        = {730--736},
doi          = {10.24963/IJCAI.2019/103},
abbr={NODEs},
abstract={Neural Ordinary Differential Equations (Neural ODEs) offer a memory-efficient way to compute gradients using the adjoint method. However, the adjoint method can be computationally expensive and numerically unstable for long integration times. We propose ANODE (Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs), a novel method for computing gradients in Neural ODEs that is both memory-efficient and numerically stable. ANODE leverages a new adjoint formulation that avoids the need for backward integration, leading to significant computational savings and improved stability. We demonstrate the effectiveness of ANODE on various tasks, showing competitive performance with existing methods while offering superior efficiency and robustness.},
paper={https://www.ijcai.org/proceedings/2019/0103.pdf},
arxiv={1903.04618},
}