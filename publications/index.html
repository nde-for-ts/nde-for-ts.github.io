<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Foundational Papers | NDE for TS </title> <meta name="author" content="NDE for TS "> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon/favicon.ico?08b761f216014d002d34580c5060f434"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nde-for-ts.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> NDE for TS </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Tutorial </a> </li> <li class="nav-item "> <a class="nav-link" href="/review/">Review </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Foundational Papers <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">Organizers </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Foundational Papers</h1> <p class="post-description"></p> </header> <article> <p>Curated list of neural differential equations for time series analysis. See comprehensive review paper <a href="https://arxiv.org/abs/2502.09885" rel="external nofollow noopener" target="_blank">here</a>.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="oh_dualdynamics_2025" class="col-sm-8"> <div class="title">DualDynamics: Synergizing Implicit and Explicit Methods for Robust Irregular Time Series Analysis</div> <div class="author"> YongKyung Oh, Dong-Young Lim, and Sungil Kim </div> <div class="periodical"> <em>In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1609/AAAI.V39I18.34173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2401.04979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/yongkyung-oh/DualDynamics" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Real-world time series analysis faces significant challenges when dealing with irregular and incomplete data. While Neural Differential Equation (NDE) based methods have shown promise, they struggle with limited expressiveness, scalability issues, and stability concerns. Conversely, Neural Flows offer stability but falter with irregular data. We introduce ’DualDynamics’, a novel framework that synergistically combines NDE-based method and Neural Flow-based method. This approach enhances expressive power while balancing computational demands, addressing critical limitations of existing techniques. We demonstrate DualDynamics’ effectiveness across diverse tasks: classification of robustness to dataset shift, irregularly-sampled series analysis, interpolation of missing data, and forecasting with partial observations. Our results show consistent outperformance over state-of-the-art methods, indicating DualDynamics’ potential to advance irregular time series analysis significantly.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#C9A227"> <div>Review</div> </abbr> </div> <div id="oh_comprehensive_2025" class="col-sm-8"> <div class="title">Comprehensive Review of Neural Differential Equations for Time Series Analysis</div> <div class="author"> YongKyung Oh, Seungsu Kam, Jonghun Lee, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Dong-Young Lim, Sungil Kim, Alex Bui' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2502.09885" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/abs/2502.09885" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2502.09885" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Differential Equations (NDEs) have emerged as a powerful paradigm for modeling complex dynamic systems, particularly in the context of time series analysis. This comprehensive review provides an in-depth exploration of the various NDE architectures, including Neural Ordinary Differential Equations (NODEs), Neural Controlled Differential Equations (NCDEs), and Neural Stochastic Differential Equations (NSDEs), and their applications in handling irregularly sampled, noisy, and high-dimensional time series data. We discuss the theoretical foundations, computational challenges, and practical considerations for implementing and training NDE-based models. Furthermore, we highlight recent advancements, open problems, and future research directions in this rapidly evolving field, aiming to provide a valuable resource for researchers and practitioners interested in leveraging NDEs for advanced time series analysis.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="oh_stable_2024" class="col-sm-8"> <div class="title">Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data</div> <div class="author"> YongKyung Oh, Dongyoung Lim, and Sungil Kim </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=4VIgNuQ1pY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2402.14989" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/yongkyung-oh/Stable-Neural-SDEs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs’ performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="jhin_attentive_2024" class="col-sm-8"> <div class="title">Attentive neural controlled differential equations for time-series classification and forecasting</div> <div class="author"> Sheo Yon Jhin, Heejoo Shin, Sujie Kim, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Seoyoung Hong, Minju Jo, Solhee Park, Noseong Park, Seungbeom Lee, Hwiyoung Maeng, Seungmin Jeon' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>Knowledge and Information Systems</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s10115-023-01977-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://link.springer.com/article/10.1007/s10115-023-01977-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2303.09020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sheoyon-jhin/Attentive-NCDE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Controlled Differential Equations (NCDEs) have shown promising results in modeling irregularly sampled time series. However, their ability to capture long-range dependencies and focus on relevant information within the time series can be limited. We propose Attentive Neural Controlled Differential Equations (Attentive NCDEs), a novel framework that integrates attention mechanisms into NCDEs. This allows the model to dynamically weigh the importance of different time points and features, enhancing its ability to capture salient information for time-series classification and forecasting. We demonstrate the effectiveness of Attentive NCDEs on various real-world datasets, showing significant improvements over state-of-the-art NCDE models, especially in complex scenarios with varying data quality and length.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="jhin_learnable_2023" class="col-sm-8"> <div class="title">Learnable Path in Neural Controlled Differential Equations</div> <div class="author"> Sheo Yon Jhin, Minju Jo, Seungji Kook, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Noseong Park' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1609/aaai.v37i7.25969" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25969" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2301.08502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sheoyon-jhin/Learnable-Path-NCDE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Controlled Differential Equations (NCDEs) have shown great promise in modeling irregularly sampled time series data. A key component of NCDEs is the path, which typically relies on linear or cubic spline interpolation of the input data. However, these fixed interpolation schemes may not be optimal for capturing complex underlying dynamics. In this paper, we propose a novel approach that introduces a ’learnable path’ within NCDEs. Our method allows the model to dynamically learn the most suitable interpolation scheme for the given data, thereby enhancing its expressiveness and adaptability. We demonstrate the effectiveness of our learnable path NCDE on various real-world datasets, showing improved performance in tasks such as time-series classification and forecasting, especially in scenarios with highly irregular observations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="li_neural_2023" class="col-sm-8"> <div class="title">Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling</div> <div class="author"> ting li, Jianguo Li, and Zhanxing Zhu </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/17345-Paper-Neural-Lad-A-Neural-Latent-Dynamics-Framework-for-Times-Series-Modeling-Abstract.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2305.18470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/tingli-tsinghua/Neural-LAD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Modeling complex time series data, especially those with irregular sampling and missing values, remains a significant challenge. Traditional methods often struggle with capturing underlying continuous dynamics and long-term dependencies. We propose Neural LAD (Neural Latent Dynamics), a novel framework that leverages latent ordinary differential equations (ODEs) to model the continuous evolution of time series in a low-dimensional latent space. Neural LAD learns a flexible, data-driven latent dynamics model that can handle irregular observations and infer missing values. We demonstrate the effectiveness of Neural LAD on various real-world datasets, including medical records and human activity recognition, showing superior performance in tasks such as imputation, forecasting, and classification compared to state-of-the-art methods.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="jhin_exit_2022" class="col-sm-8"> <div class="title">EXIT: Extrapolation and Interpolation-based Neural Controlled Differential Equations for Time-series Classification and Forecasting.</div> <div class="author"> Sheo Yon Jhin, Jaehoon Lee, Minju Jo, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Seungji Kook, Jinsung Jeon, Jihyeon Hyeong, Jayoung Kim, Noseong Park' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In WWW ’22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3485447.3512030" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512030" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2203.09020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/sheoyon-jhin/EXIT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Controlled Differential Equations (NCDEs) have emerged as a powerful tool for modeling irregularly sampled time series data. However, existing NCDE models often struggle with extrapolation and interpolation tasks, which are crucial for real-world applications like time-series classification and forecasting. To address these limitations, we propose EXIT (Extrapolation and Interpolation-based Neural Controlled Differential Equations), a novel framework that enhances NCDEs’ capabilities in handling both extrapolation and interpolation. EXIT introduces a dual-path architecture that explicitly models both the observed data points and the underlying continuous dynamics, allowing for more robust and accurate predictions. We demonstrate the effectiveness of EXIT on various real-world datasets, showing significant improvements over state-of-the-art NCDE models in time-series classification and forecasting tasks, especially in scenarios with high irregularity and missing data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="morrill_choice_2022" class="col-sm-8"> <div class="title">On the Choice of Interpolation Scheme for Neural CDEs</div> <div class="author"> James Morrill, Patrick Kidger, Lingyi Yang, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Terry Lyons' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://jmlr.org/papers/v23/21-0902.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2109.00891" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Controlled Differential Equations (Neural CDEs) are a powerful model for irregularly-sampled time series. A key component of Neural CDEs is the choice of interpolation scheme for the input data. Common choices include linear interpolation and cubic spline interpolation. We investigate the impact of different interpolation schemes on the performance of Neural CDEs. We show that the choice of interpolation scheme can significantly affect the model’s ability to capture complex dynamics and generalize to unseen data. We provide theoretical insights and empirical evidence on various real-world datasets, demonstrating that a carefully chosen interpolation scheme can lead to substantial performance improvements.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="morrill_neural_2021" class="col-sm-8"> <div class="title">Neural Rough Differential Equations for Long Time Series.</div> <div class="author"> James Morrill, Cristopher Salvi, Patrick Kidger, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'James Foster' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event.</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v139/morrill21a/morrill21a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2106.01311" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/patrickkidger/NeuralRDE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Controlled Differential Equations (CDEs) are a powerful model for irregularly-sampled time series. However, their computational cost scales quadratically with the length of the time series, making them impractical for long sequences. We propose Neural Rough Differential Equations (RDEs), a new model that leverages the theory of rough paths to reduce the computational cost of CDEs to linear in the length of the time series. Our model retains the expressiveness of CDEs while being significantly more efficient. We demonstrate the effectiveness of our model on several real-world datasets, including human motion capture data and medical records. Our model outperforms existing methods for long irregularly-sampled time series.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="song_score-based_2021" class="col-sm-8"> <div class="title">Score-Based Generative Modeling through Stochastic Differential Equations.</div> <div class="author"> Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Abhishek Kumar, Stefano Ermon, Ben Poole' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=PxTIG12RRf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2011.13456" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/yang-song/score_sde" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>We propose a unified framework for generative modeling that leverages stochastic differential equations (SDEs) to transform a simple prior distribution into a complex data distribution. Our framework encompasses various existing generative models, including score-based generative models and denoising diffusion probabilistic models, as special cases. By defining a continuous-time SDE that gradually perturbs data towards noise, and its reverse-time SDE that transforms noise back into data, we can generate samples by solving the reverse SDE. We demonstrate the effectiveness of our framework on various tasks, including image generation and unconditional sampling, showing competitive performance with state-of-the-art generative models.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B23A48"> <div>NCDEs</div> </abbr> </div> <div id="kidger_neural_2020" class="col-sm-8"> <div class="title">Neural Controlled Differential Equations for Irregular Time Series.</div> <div class="author"> Patrick Kidger, James Morrill, James Foster, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Terry J. Lyons' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/file/6e1366887956979f3326740908976477-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2005.08926" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/patrickkidger/NeuralCDE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Many real-world time series are irregularly sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks are not well-suited to this setting, as they assume fixed time intervals between observations. We propose a new model for irregularly-sampled time series based on Neural Controlled Differential Equations (CDEs). Our model learns the continuous-time dynamics of the underlying system, and can be evaluated at arbitrary time points. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data. Our model outperforms state-of-the-art methods for irregularly-sampled time series, and is competitive with methods for regularly-sampled data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="li_scalable_2020" class="col-sm-8"> <div class="title">Scalable Gradients for Stochastic Differential Equations</div> <div class="author"> Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'David Duvenaud' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</em>, Aug 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v108/li20a/li20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2002.09544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/google-research/torchsde" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Stochastic differential equations (SDEs) are a powerful tool for modeling systems with inherent noise. However, training models that involve SDEs can be challenging due to the difficulty of computing gradients through stochastic processes. We propose a new method for computing gradients of expectations with respect to SDEs that is both scalable and unbiased. Our method leverages the adjoint method, which allows us to compute gradients by solving a backward SDE. We demonstrate the effectiveness of our method on several tasks, including training generative models and learning latent SDEs from data. Our method outperforms existing approaches in terms of both accuracy and computational efficiency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="lechner_learning_2020" class="col-sm-8"> <div class="title">Learning Long-Term Dependencies in Irregularly-Sampled Time Series</div> <div class="author"> Mathias Lechner and Ramin Hasani </div> <div class="periodical"> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2006.04418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/abs/2006.04418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2006.04418" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/mlech26l/ode-lstms" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Many real-world time series are irregularly sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks struggle with such data due to their reliance on fixed time steps. We propose a novel approach for learning long-term dependencies in irregularly-sampled time series by combining the strengths of recurrent neural networks with continuous-time models. Our method leverages a continuous-time hidden state that evolves according to a learned ordinary differential equation, allowing it to capture complex temporal dynamics regardless of sampling irregularity. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data, showing improved performance in tasks such as classification and forecasting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="ghosh_steer_2020" class="col-sm-8"> <div class="title">STEER : Simple Temporal Regularization For Neural ODE</div> <div class="author"> Arnab Ghosh, Harkirat Behl, Emilien Dupont, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Philip Torr, Vinay Namboodiri' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/file/58d4d1e7b1e97b252200c512f91e0edb-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) have emerged as a powerful tool for modeling continuous-time dynamics. However, training Neural ODEs can be challenging due to their sensitivity to initial conditions and the lack of explicit regularization for temporal smoothness. We propose STEER (Simple Temporal Regularization For Neural ODE), a novel regularization technique that encourages temporal smoothness in the learned dynamics of Neural ODEs. STEER adds a penalty term to the loss function that regularizes the magnitude of the derivative of the hidden state, promoting more stable and interpretable dynamics. We demonstrate the effectiveness of STEER on various tasks, including time series modeling and density estimation, showing improved performance and robustness compared to standard Neural ODEs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="zhuang_adaptive_2020" class="col-sm-8"> <div class="title">Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE</div> <div class="author"> Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Sekhar Tatikonda, Xenophon Papademetris, James Duncan' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 37th International Conference on Machine Learning</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v119/zhuang20a/zhuang20a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2006.02448" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) offer a memory-efficient way to model continuous-time dynamics by backpropagating through an ODE solver using the adjoint method. However, the adjoint method can be computationally expensive, especially for long integration times, as it requires solving an additional backward ODE. We propose the Adaptive Checkpoint Adjoint Method (ACAM), a novel approach that combines checkpointing with the adjoint method to reduce memory and computational costs. ACAM adaptively selects checkpoints during the forward pass, allowing for efficient gradient computation without storing the entire trajectory. We demonstrate the effectiveness of ACAM on various tasks, showing significant reductions in memory and computation while maintaining accuracy compared to standard adjoint methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="zhang_approximation_2020" class="col-sm-8"> <div class="title">Approximation Capabilities of Neural ODEs and Invertible Residual Networks</div> <div class="author"> Han Zhang, Xi Gao, Jacob Unterman, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Tom Arodz' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 37th International Conference on Machine Learning</em>, Jul 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v119/zhang20c/zhang20c.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2006.02448" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) and Invertible Residual Networks (i-ResNets) are two recent architectures that offer memory-efficient training and exact likelihood computation, respectively. Both models are based on continuous-depth transformations. We investigate the approximation capabilities of Neural ODEs and i-ResNets, showing that they are universal approximators of continuous functions. We provide theoretical guarantees on their ability to approximate arbitrary continuous functions and demonstrate their practical performance on various tasks, including density estimation and image classification. Our results shed light on the expressive power of these continuous-depth models and their potential for deep learning applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="teshima_universal_2020" class="col-sm-8"> <div class="title">Universal Approximation Property of Neural Ordinary Differential Equations</div> <div class="author"> Takeshi Teshima, Koichi Tojo, Masahiro Ikeda, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Isao Ishikawa, Kenta Oono' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2012.02414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/abs/2012.02414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2012.02414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) have recently emerged as a new class of deep learning models that parameterize the derivative of the hidden state using a neural network. This allows them to model continuous-time dynamics and offer memory-efficient training. We prove that Neural ODEs possess the universal approximation property, meaning they can approximate any continuous function to arbitrary accuracy. Our proof provides theoretical guarantees on the expressive power of Neural ODEs and sheds light on their ability to learn complex mappings. This result further solidifies the theoretical foundation of Neural ODEs and supports their wide applicability in various domains.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="lou_neural_2020" class="col-sm-8"> <div class="title">Neural Manifold Ordinary Differential Equations</div> <div class="author"> Aaron Lou, Derek Lim, Isay Katsman, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Leo Huang, Qingxuan Jiang, Ser Nam Lim, Christopher M De Sa' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/file/229699d324817122c560356723229718-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2006.08445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/louaaron/neural-manifold-odes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) have shown promise in modeling continuous-time dynamics. However, they operate in Euclidean space, which may not be suitable for data residing on non-Euclidean manifolds. We introduce Neural Manifold Ordinary Differential Equations (Neural Manifold ODEs), a novel framework that extends Neural ODEs to learn dynamics on Riemannian manifolds. Our approach leverages geometric deep learning techniques to define ODEs on curved spaces, allowing for more natural and expressive modeling of data with underlying manifold structures. We demonstrate the effectiveness of Neural Manifold ODEs on tasks such as learning dynamics on spheres and hyperbolic spaces, showing improved performance and interpretability compared to Euclidean counterparts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="massaroli_dissecting_2020" class="col-sm-8"> <div class="title">Dissecting Neural ODEs</div> <div class="author"> Stefano Massaroli, Michael Poli, Jinkyoo Park, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Atsushi Yamashita, Hajime Asama' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/file/3952-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/2006.07366" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/massaroli/Dissecting-Neural-ODEs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) have gained significant attention for their ability to model continuous-time dynamics and offer memory-efficient training. However, their internal workings and the factors influencing their performance are not yet fully understood. We conduct a thorough dissection of Neural ODEs, analyzing various aspects such as their approximation capabilities, sensitivity to hyperparameters, and the role of the ODE solver. Through extensive empirical studies and theoretical insights, we provide a deeper understanding of why Neural ODEs work and how to effectively train them. Our findings offer practical guidelines for designing and applying Neural ODEs in various machine learning tasks.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="rubanova_latent_2019" class="col-sm-8"> <div class="title">Latent Ordinary Differential Equations for Irregularly-Sampled Time Series.</div> <div class="author"> Yulia Rubanova, Tian Qi Chen, and David Duvenaud </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada.</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/432a7e5a8e4864265426e6219927a69e-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1907.03907" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/YuliaRubanova/latent_ode" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Many real-world time series are irregularly-sampled, with observations arriving at arbitrary time points. Standard recurrent neural networks are not well-suited to this setting, as they assume fixed time intervals between observations. We propose a new model for irregularly-sampled time series based on Neural Ordinary Differential Equations (ODEs). Our model learns the continuous-time dynamics of the underlying system, and can be evaluated at arbitrary time points. We demonstrate the effectiveness of our model on several real-world datasets, including medical records and human motion capture data. Our model outperforms state-of-the-art methods for irregularly-sampled time series, and is competitive with methods for regularly-sampled data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="dupont_augmented_2019" class="col-sm-8"> <div class="title">Augmented Neural ODEs.</div> <div class="author"> Emilien Dupont, Arnaud Doucet, and Yee Whye Teh </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada.</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2019/file/2d107313a774780753067311b739955e-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1904.01681" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/EmilienDupont/augmented-neural-odes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) parameterize the derivative of the hidden state using a neural network. This allows them to model continuous-time dynamics and handle irregularly sampled data. However, Neural ODEs are limited by the fact that the hidden state must have the same dimension as the input. We propose Augmented Neural ODEs (ANODEs), which augment the hidden state with additional dimensions. This allows ANODEs to learn more complex dynamics and improve performance on tasks such as density estimation and time series modeling. We demonstrate the effectiveness of ANODEs on several real-world datasets, showing significant improvements over standard Neural ODEs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="brouwer_gru-ode-bayes_2019" class="col-sm-8"> <div class="title">GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series.</div> <div class="author"> Edward De Brouwer, Jaak Simm, Adam Arany, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yves Moreau' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada.</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2019/file/7377200700f909599e03099ad1139cb3-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1905.13221" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/edebrouwer/GRU-ODE-Bayes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Many real-world time series are sporadically observed, meaning that observations arrive at irregular time intervals and some values may be missing. Standard recurrent neural networks struggle with such data due to their reliance on fixed time steps. We propose GRU-ODE-Bayes, a novel model that combines the strengths of Gated Recurrent Units (GRUs) with Ordinary Differential Equations (ODEs) and Bayesian inference to handle sporadically observed time series. Our model learns continuous-time dynamics and provides uncertainty estimates for predictions. We demonstrate the effectiveness of GRU-ODE-Bayes on several real-world datasets, including medical records and climate data, showing significant improvements over state-of-the-art methods for irregularly sampled time series.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="tzen_neural_2019" class="col-sm-8"> <div class="title">Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit</div> <div class="author"> Belinda Tzen and Maxim Raginsky </div> <div class="periodical"> Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.1905.09883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/abs/1905.09883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1905.09883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We introduce Neural Stochastic Differential Equations (Neural SDEs), a new class of deep generative models that extend Neural Ordinary Differential Equations (Neural ODEs) by incorporating a stochastic diffusion term. Neural SDEs model continuous-time dynamics with inherent noise, allowing for more robust and expressive representations of complex data. We show that Neural SDEs can be viewed as deep latent Gaussian models in the diffusion limit, providing a theoretical foundation for their generative capabilities. We demonstrate the effectiveness of Neural SDEs on tasks such as density estimation and time series modeling, showing competitive performance with state-of-the-art methods while offering improved flexibility and interpretability.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="yildiz_ode2vae_2019" class="col-sm-8"> <div class="title">ODE2VAE: Deep generative second order ODEs with Bayesian neural networks</div> <div class="author"> Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2019/file/202cb962ac59075b964b07152d234b70-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/cagatayyildiz/ODE2VAE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Deep generative models have achieved remarkable success in various domains, but modeling complex temporal dynamics, especially with irregularly sampled data, remains a challenge. We propose ODE2VAE, a novel deep generative model that combines second-order Ordinary Differential Equations (ODEs) with Variational Autoencoders (VAEs) and Bayesian neural networks. Our model learns continuous-time dynamics in the latent space, allowing it to handle irregular observations and capture complex temporal dependencies. The use of second-order ODEs enables modeling of acceleration and momentum, providing richer dynamics. We demonstrate the effectiveness of ODE2VAE on several real-world datasets, including medical records and motion capture data, showing improved performance in tasks such as imputation, forecasting, and anomaly detection.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="liu_neural_2019" class="col-sm-8"> <div class="title">Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise</div> <div class="author"> Xuanqing Liu, Tesi Xiao, Si Si, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/ARXIV.1906.02355" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/abs/1906.02355" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1906.02355" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) have gained popularity for modeling continuous-time dynamics. However, they can be sensitive to noise and prone to instability, especially in high-dimensional settings. We propose Neural SDE, a novel approach that stabilizes Neural ODE networks by introducing stochastic noise into the dynamics. By augmenting the deterministic ODE with a learnable diffusion term, Neural SDE can better capture uncertainties and improve robustness. We demonstrate the effectiveness of Neural SDE on various tasks, including density estimation and time series modeling, showing improved stability and performance compared to standard Neural ODEs, particularly in the presence of noisy data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="jia_neural_2019" class="col-sm-8"> <div class="title">Neural Jump Stochastic Differential Equations</div> <div class="author"> Junteng Jia and Austin R Benson </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2019/file/21c7069d0ee8779496879895-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1905.09883" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Stochastic differential equations (SDEs) are widely used to model systems with continuous-time random fluctuations. However, many real-world phenomena exhibit sudden, discrete changes or jumps, which are not captured by standard SDEs. We introduce Neural Jump Stochastic Differential Equations (Neural JSDEs), a novel class of models that extend Neural SDEs by incorporating jump processes. Neural JSDEs can model both continuous diffusion and discrete jumps, providing a more comprehensive framework for complex temporal dynamics. We demonstrate the effectiveness of Neural JSDEs on various tasks, including financial modeling and anomaly detection, showing improved performance in scenarios with abrupt changes.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="behrmann_invertible_2019" class="col-sm-8"> <div class="title">Invertible Residual Networks</div> <div class="author"> Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'David Duvenaud, Joern-Henrik Jacobsen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 36th International Conference on Machine Learning</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1811.00995" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/jhjacobsen/invertible-resnet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>We introduce Invertible Residual Networks (i-ResNets), a new class of deep neural networks that are invertible by construction. This property allows for exact likelihood computation and memory-efficient training, as activations do not need to be stored for backpropagation. i-ResNets are built upon the idea of residual blocks, where each block is designed to be invertible. We demonstrate the effectiveness of i-ResNets on various tasks, including image classification and density estimation, showing competitive performance with state-of-the-art models while offering significant memory savings. Our work opens up new possibilities for designing deep architectures with desirable properties.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="grathwohl_ffjord_2019" class="col-sm-8"> <div class="title">FFJORD: Free-Form Continuous Dynamics for Scalable Reversible Generative Models.</div> <div class="author"> Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ilya Sutskever, David Duvenaud' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=rJg6CVAcf7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1810.01367" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/rtqichen/ffjord" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>Normalizing flows are a powerful class of generative models that allow for exact likelihood computation. However, existing normalizing flows often suffer from limitations in expressiveness or scalability. We introduce FFJORD (Free-Form Jacobian of Reversible Dynamics), a novel normalizing flow model based on Neural Ordinary Differential Equations (Neural ODEs). FFJORD parameterizes the transformation as a continuous-time dynamic system, allowing for flexible and expressive transformations while maintaining invertibility and exact likelihood computation. We demonstrate the scalability and effectiveness of FFJORD on various tasks, including density estimation and image generation, showing competitive performance with state-of-the-art generative models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="gholaminejad_anode_2019" class="col-sm-8"> <div class="title">ANODE: Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs.</div> <div class="author"> Amir Gholaminejad, Kurt Keutzer, and George Biros </div> <div class="periodical"> <em>In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019</em>, Oct 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.24963/IJCAI.2019/103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://www.ijcai.org/proceedings/2019/0103.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1903.04618" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Neural Ordinary Differential Equations (Neural ODEs) offer a memory-efficient way to compute gradients using the adjoint method. However, the adjoint method can be computationally expensive and numerically unstable for long integration times. We propose ANODE (Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs), a novel method for computing gradients in Neural ODEs that is both memory-efficient and numerically stable. ANODE leverages a new adjoint formulation that avoids the need for backward integration, leading to significant computational savings and improved stability. We demonstrate the effectiveness of ANODE on various tasks, showing competitive performance with existing methods while offering superior efficiency and robustness.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2A4B8D"> <div>NODEs</div> </abbr> </div> <div id="chen_neural_2018" class="col-sm-8"> <div class="title">Neural Ordinary Differential Equations.</div> <div class="author"> Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'David Duvenaud' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada.</em>, Oct 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2018/file/6ad323483731627375b420a359b99a74-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1806.07366" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/rtqichen/torchdiffeq" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">GitHub</a> </div> <div class="abstract hidden"> <p>We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed by an off-the-shelf ODE solver. These ’Neural ODEs’ can model continuous-time dynamics, and can be trained with backpropagation through the ODE solver. This allows for memory-efficient training, as the adjoint method can be used to compute gradients without storing the entire forward pass. We demonstrate the effectiveness of Neural ODEs on various tasks, including density estimation, time series modeling, and image classification, showing competitive performance with state-of-the-art models while offering significant memory savings.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2F8F6F"> <div>NSDEs</div> </abbr> </div> <div id="e_deep_2017" class="col-sm-8"> <div class="title">Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations</div> <div class="author"> Weinan E, Jiequn Han, and Arnulf Jentzen </div> <div class="periodical"> <em>Communications in Mathematics and Statistics</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s40304-017-0117-6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://link.springer.com/article/10.1007/s40304-017-0117-6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="http://arxiv.org/abs/1707.02568" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>We propose a deep learning-based numerical method for solving high-dimensional parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs). The method is based on approximating the unknown function by a deep neural network and training the network by minimizing a loss function derived from the PDE or BSDE. We demonstrate the effectiveness of our method on several high-dimensional examples, including the Black-Scholes equation and a high-dimensional Hamilton-Jacobi-Bellman equation. Our method outperforms traditional numerical methods in terms of accuracy and computational efficiency for high-dimensional problems.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>